{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "StructureAdaptionFramework: a framework for handling neuron-level and layer-level structure adaptions in\n",
    "neural networks.\n",
    "\n",
    "Copyright (C) 2023  Roman Frels, roman.frels@gmail.com\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, version 3 of the License.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non sequential struture adaption with training example\n",
    "\n",
    "In this example we will combine structure adaption with training. It's important to keep in mind, that each structure adaption initializes a new internally wrapped tensorflow model. It behaves like every other tensorflow model, but all references to the previous internal model are invalidated and need to be updated. This includes usually the optimizer slots, referring to weights of the model, and the grow prun tuples, sequential branches and parallel branches.\n",
    "This example includes:\n",
    "- Adding and removing neurons while training the network with an optimizer in between\n",
    "- Adding and removing branches while training the network with an optimizer in between\n",
    "\n",
    "To understand more caveats please refer to the documentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import StructureAdaption.structure_adaption as structure_adaption\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T07:49:06.050484Z",
     "end_time": "2023-09-06T07:49:06.316997Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simplicity we use the simple dense model from the first example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x0 (InputLayer)             [(None, 10)]              0         \n",
      "                                                                 \n",
      " x1 (AdaptionLayer)          (None, 6)                 66        \n",
      "                                                                 \n",
      " x2 (AdaptionLayer)          (None, 2)                 14        \n",
      "                                                                 \n",
      " x3 (AdaptionLayer)          (None, 4)                 12        \n",
      "                                                                 \n",
      " x4 (Softmax)                (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 92\n",
      "Trainable params: 92\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_features = 10\n",
    "\n",
    "def example_model():\n",
    "    inputs = tf.keras.Input(shape=[num_features], dtype=tf.dtypes.float32, name='x0')\n",
    "    x1 = tf.keras.layers.Dense(units=6, activation='relu', name='x1')(inputs)\n",
    "    x2 = tf.keras.layers.Dense(units=2, activation='relu', name='x2')(x1)\n",
    "    x3 = tf.keras.layers.Dense(units=4, activation='relu', name='x3')(x2)\n",
    "    outputs = tf.keras.layers.Softmax(name='x4')(x3)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "base_model = example_model()\n",
    "parsed_model = structure_adaption.parse_model(base_model)\n",
    "\n",
    "def compile_fn():\n",
    "    parsed_model(tf.random.uniform(shape=(1, num_features)))\n",
    "    #parsed_model(tf.keras.Input((num_features)))\n",
    "\n",
    "parsed_model.summary()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T07:49:06.067634Z",
     "end_time": "2023-09-06T07:49:06.402798Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to define an optimizer and a training set for training. We train the model on the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.6965 - accuracy: 0.7396\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.6696 - accuracy: 0.9062\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.6407 - accuracy: 0.9896\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.6100 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.5768 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.5411 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.5010 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.4569 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.4088 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.3567 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f90605a2250>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = 96\n",
    "random_features = tf.random.uniform(shape=(num_samples, num_features))\n",
    "random_label = tf.math.reduce_mean(random_features, axis=1, keepdims=True)\n",
    "random_labels = tf.math.exp(tf.concat([random_label, 2* random_label, 3* random_label, 4*random_label], axis=1))\n",
    "dataset = tf.data.Dataset.from_tensor_slices((random_features, random_labels))\n",
    "dataset = dataset.batch(32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.01)  #TODO #jit_compile=False)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "parsed_model.internal_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "parsed_model.internal_model.fit(dataset, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T07:49:06.195595Z",
     "end_time": "2023-09-06T07:49:06.835652Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We remove two neurons from layer x1. After that we need to recompile the model and we train again. The optimizer slots are carried over automatically. We grow now two neurons in layer x2 and train again afterwards. In between growing and pruning the internal model can be treated like a regular tensorflow model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of grow prun tuples: 2\n",
      "Name of the layer of the first gp tuple: x1\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.6591 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.6405 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.6210 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.6006 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.5795 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.5573 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.5342 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.5096 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.4832 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.4556 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.4271 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.3971 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.3657 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.3329 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.2986 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.2630 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.2251 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.1856 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.1446 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 22.1023 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f9060413160>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_tuples = parsed_model.grow_prun_tuples\n",
    "first = gp_tuples[0]\n",
    "print('number of grow prun tuples: ' + str(len(gp_tuples)))\n",
    "print(\"Name of the layer of the first gp tuple: \" + gp_tuples[0].first_layer.name)\n",
    "parsed_model.prun(first, [3, 5], optimizer, compile_fn=compile_fn)\n",
    "parsed_model.internal_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "parsed_model.internal_model.fit(dataset, epochs=10)\n",
    "gp_tuples = parsed_model.grow_prun_tuples\n",
    "second = gp_tuples[1]\n",
    "zeros_init = tf.keras.initializers.Zeros()\n",
    "init_dict = {'first': [zeros_init, zeros_init], 'last': [zeros_init, zeros_init]}\n",
    "parsed_model.grow(second, 2, init_dict, optimizer, compile_fn)\n",
    "parsed_model.internal_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "parsed_model.internal_model.fit(dataset, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T07:49:06.730821Z",
     "end_time": "2023-09-06T07:49:07.812290Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As after every growing or pruning step we need to get the new sequential branches again from the parsed model. We grow a new branch from layer x0 to layer x3, introducing a new add node in the process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " x1 (AdaptionLayer)             (None, 4)            44          ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x5 (AdaptionLayer)             (None, 2)            22          ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x2 (AdaptionLayer)             (None, 4)            20          ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " x6 (AdaptionLayer)             (None, 4)            12          ['x5[0][0]']                     \n",
      "                                                                                                  \n",
      " new_add_node0 (Add)            (None, 4)            0           ['x2[0][0]',                     \n",
      "                                                                  'x6[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (AdaptionLayer)             (None, 4)            20          ['new_add_node0[0][0]']          \n",
      "                                                                                                  \n",
      " x4 (Softmax)                   (None, 4)            0           ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 22.0417 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.9928 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.9440 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.8964 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.8504 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.8063 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.7646 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.7257 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.6899 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.6569 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f90603cb820>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_branch = parsed_model.sequential_branches[0]\n",
    "insert_start =  sequential_branch.layers[0]\n",
    "insert_end = sequential_branch.layers[3]\n",
    "l5 = tf.keras.layers.Dense(units=2, activation='relu', name='x5')\n",
    "x5 = l5(insert_start.output)\n",
    "l6 = tf.keras.layers.Dense(units=4, activation='relu', name='x6')\n",
    "x6 = l6(x5)\n",
    "\n",
    "grow_branch = structure_adaption.InsertBranch([insert_start, l5, l6], insert_end)\n",
    "parsed_model.grow_branch(grow_branch, optimizer, compile_fn, carry_optimizer=True)\n",
    "parsed_model.summary()\n",
    "parsed_model.internal_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "parsed_model.internal_model.fit(dataset, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T07:49:07.789796Z",
     "end_time": "2023-09-06T07:49:08.442400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We prun the branch over the layers x0, x1, and x2 and leave a residual connection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prun branch consisting of layers: \n",
      "x0\n",
      "x5\n",
      "x6\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " x2 (AdaptionLayer)             (None, 4)            44          ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x6 (AdaptionLayer)             (None, 4)            44          ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " new_add_node0 (Add)            (None, 4)            0           ['x2[0][0]',                     \n",
      "                                                                  'x6[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (AdaptionLayer)             (None, 4)            20          ['new_add_node0[0][0]']          \n",
      "                                                                                                  \n",
      " x4 (Softmax)                   (None, 4)            0           ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108\n",
      "Trainable params: 108\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.5749 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.5524 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.5323 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.5144 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.4983 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.4839 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.4708 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.4590 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 21.4485 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 21.4390 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f906026eb80>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = parsed_model.internal_model.layers\n",
    "prun_branch = structure_adaption.Branch([layers[0], layers[1], layers[3]])\n",
    "print('Prun branch consisting of layers: ')\n",
    "for layer in prun_branch.layers:\n",
    "    print(layer.name)\n",
    "parsed_model.prun_branch(prun_branch, optimizer, compile_fn, carry_optimizer=True, leave_residual=True, skip_mismatch=True)\n",
    "parsed_model.summary()\n",
    "parsed_model.internal_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "parsed_model.internal_model.fit(dataset, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T07:54:04.222469Z",
     "end_time": "2023-09-06T07:54:04.794152Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we have seen between growing and pruning the internal model can be treated as any other tensorflow model. No restrictions apply here. When growing or pruning the optimizer should be provided to carry over optimizer slots to the cloned network. Keeping in mind that the network is cloned in each adaption step should hint at the caveats that apply. For further details consult the documentation."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
