{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "StructureAdaptionFramework: a framework for handling neuron-level and layer-level structure adaptions in\n",
    "neural networks.\n",
    "\n",
    "Copyright (C) 2023  Roman Frels, roman.frels@gmail.com\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published by\n",
    "the Free Software Foundation, version 3 of the License.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Non sequential structure adaption example\n",
    "\n",
    "In the first example of basic structure adaption we have seen removing and adding neurons and layers sequentially. In this example we will remove and add layers non-sequentially and observe more caveats of the structure adaption framework.\n",
    "This example includes:\n",
    "- Sequential, parallel, and sequential-parallel branches\n",
    "- Grow prun tuples and their relation to sequential branches\n",
    "- Adding and removing neurons with intermediate layers\n",
    "- Adding and removing branches non-sequentially\n",
    "\n",
    "To understand the caveats arising in combination with training, see the training focused examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 18:23:54.030460: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-29 18:23:54.030516: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import StructureAdaption.structure_adaption as structure_adaption\n",
    "import numpy as np\n",
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T18:23:53.011316Z",
     "end_time": "2023-08-29T18:23:59.429236Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time we define a convolutional network as our example model. It has a parallel path, which does not necessarily make sense. We take care that the dimensions for the add layer match."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def example_model():\n",
    "    inputs = tf.keras.Input(shape=[32, 32, 3], dtype=tf.dtypes.float32, name='x0')\n",
    "    x1 = tf.keras.layers.Conv2D(6, 3, strides=1, padding='same', activation='relu', name='x1')(inputs)\n",
    "    x2 = tf.keras.layers.BatchNormalization(name='x2')(x1)\n",
    "    x3 = tf.keras.layers.Conv2D(6, 3, strides=2, padding='valid', activation='relu', name='x3')(x2)\n",
    "    x4 = tf.keras.layers.BatchNormalization(name='x4')(x3)\n",
    "\n",
    "    x5 = tf.keras.layers.Conv2D(6, 3, strides=1, padding='same', activation='relu', name='x5')(x4)\n",
    "    x6 = tf.keras.layers.BatchNormalization(name='x6')(x5)\n",
    "    x7 = tf.keras.layers.Conv2D(6, 3, strides=2, padding='valid', activation='relu', name='x7')(x6)\n",
    "\n",
    "    x8 = tf.keras.layers.Conv2D(6, 3, strides=1, padding='same', activation='relu', name='x8')(x4)\n",
    "    x9 = tf.keras.layers.BatchNormalization(name='x9')(x8)\n",
    "    x10 = tf.keras.layers.Conv2D(6, 3, strides=2, padding='valid', activation='relu', name='x10')(x9)\n",
    "\n",
    "    x11 = tf.keras.layers.Add(name='x11')([x7, x10])\n",
    "\n",
    "    x12 = tf.keras.layers.BatchNormalization(name='x12')(x11)\n",
    "    x13 = tf.keras.layers.Conv2D(9, 3, strides=1, padding='same', activation='relu', name='x13')(x12)\n",
    "    x14 = tf.keras.layers.BatchNormalization(name='x14')(x13)\n",
    "    x15 = tf.keras.layers.Conv2D(6, 3, strides=2, padding='valid', activation='relu', name='x15')(x14)\n",
    "\n",
    "    x16 = tf.keras.layers.Flatten(name='x16')(x15)\n",
    "    x17 = tf.keras.layers.Dense(units=2, activation='relu', name='x17')(x16)\n",
    "    outputs = tf.keras.layers.Softmax(name='x18')(x17)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T18:23:59.428241Z",
     "end_time": "2023-08-29T18:23:59.492811Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are parsing the model and retrieve the grow prun truples, the sequential branches and the parallel branches. We can see that there are four, four and one of them respectively. This means the model has four supported layers with consecutive supported layers that are eligible to neuron-level adaptions (neuron-level growing and pruning). The BatchNormalization layers conserve the output dimension and form the intermediate layers of the grow prun tuples. The model has furthermore four purely sequential portions/branches and one portion, where different branches form parallel paths."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 18:23:59.495369: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-08-29 18:23:59.495434: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-29 18:23:59.495473: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rome): /proc/driver/nvidia/version does not exist\n",
      "2023-08-29 18:23:59.496017: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " x1 (Conv2D)                    (None, 32, 32, 6)    168         ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x2 (BatchNormalization)        (None, 32, 32, 6)    24          ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (Conv2D)                    (None, 15, 15, 6)    330         ['x2[0][0]']                     \n",
      "                                                                                                  \n",
      " x4 (BatchNormalization)        (None, 15, 15, 6)    24          ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      " x5 (Conv2D)                    (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x8 (Conv2D)                    (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x6 (BatchNormalization)        (None, 15, 15, 6)    24          ['x5[0][0]']                     \n",
      "                                                                                                  \n",
      " x9 (BatchNormalization)        (None, 15, 15, 6)    24          ['x8[0][0]']                     \n",
      "                                                                                                  \n",
      " x7 (Conv2D)                    (None, 7, 7, 6)      330         ['x6[0][0]']                     \n",
      "                                                                                                  \n",
      " x10 (Conv2D)                   (None, 7, 7, 6)      330         ['x9[0][0]']                     \n",
      "                                                                                                  \n",
      " x11 (Add)                      (None, 7, 7, 6)      0           ['x7[0][0]',                     \n",
      "                                                                  'x10[0][0]']                    \n",
      "                                                                                                  \n",
      " x12 (BatchNormalization)       (None, 7, 7, 6)      24          ['x11[0][0]']                    \n",
      "                                                                                                  \n",
      " x13 (Conv2D)                   (None, 7, 7, 9)      495         ['x12[0][0]']                    \n",
      "                                                                                                  \n",
      " x14 (BatchNormalization)       (None, 7, 7, 9)      36          ['x13[0][0]']                    \n",
      "                                                                                                  \n",
      " x15 (Conv2D)                   (None, 3, 3, 6)      492         ['x14[0][0]']                    \n",
      "                                                                                                  \n",
      " x16 (Flatten)                  (None, 54)           0           ['x15[0][0]']                    \n",
      "                                                                                                  \n",
      " x17 (Dense)                    (None, 2)            110         ['x16[0][0]']                    \n",
      "                                                                                                  \n",
      " x18 (Softmax)                  (None, 2)            0           ['x17[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,071\n",
      "Trainable params: 2,993\n",
      "Non-trainable params: 78\n",
      "__________________________________________________________________________________________________\n",
      "number of grow prun tuples: 4\n",
      "number of sequential branches: 4\n",
      "number of parallel branches: 1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " x1 (AdaptionLayer)             (None, 32, 32, 6)    168         ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x2 (AdaptionLayer)             (None, 32, 32, 6)    24          ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x2[0][0]']                     \n",
      "                                                                                                  \n",
      " x4 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      " x5 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x8 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x6 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x5[0][0]']                     \n",
      "                                                                                                  \n",
      " x9 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x8[0][0]']                     \n",
      "                                                                                                  \n",
      " x7 (AdaptionLayer)             (None, 7, 7, 6)      330         ['x6[0][0]']                     \n",
      "                                                                                                  \n",
      " x10 (AdaptionLayer)            (None, 7, 7, 6)      330         ['x9[0][0]']                     \n",
      "                                                                                                  \n",
      " x11 (Add)                      (None, 7, 7, 6)      0           ['x7[0][0]',                     \n",
      "                                                                  'x10[0][0]']                    \n",
      "                                                                                                  \n",
      " x12 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x11[0][0]']                    \n",
      "                                                                                                  \n",
      " x13 (AdaptionLayer)            (None, 7, 7, 9)      495         ['x12[0][0]']                    \n",
      "                                                                                                  \n",
      " x14 (AdaptionLayer)            (None, 7, 7, 9)      36          ['x13[0][0]']                    \n",
      "                                                                                                  \n",
      " x15 (AdaptionLayer)            (None, 3, 3, 6)      492         ['x14[0][0]']                    \n",
      "                                                                                                  \n",
      " x16 (Flatten)                  (None, 54)           0           ['x15[0][0]']                    \n",
      "                                                                                                  \n",
      " x17 (AdaptionLayer)            (None, 2)            110         ['x16[0][0]']                    \n",
      "                                                                                                  \n",
      " x18 (Softmax)                  (None, 2)            0           ['x17[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,071\n",
      "Trainable params: 2,993\n",
      "Non-trainable params: 78\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = example_model()\n",
    "base_model.summary()\n",
    "\n",
    "parsed_model = structure_adaption.parse_model(base_model)\n",
    "gp_tuples = parsed_model.grow_prun_tuples\n",
    "sequential_branches = parsed_model.sequential_branches\n",
    "parallel_branches = parsed_model.parallel_branches\n",
    "print('number of grow prun tuples: ' + str(len(gp_tuples)))\n",
    "print('number of sequential branches: ' + str(len(sequential_branches)))\n",
    "print('number of parallel branches: ' + str(len(parallel_branches)))\n",
    "\n",
    "def compile_fn():\n",
    "    parsed_model(tf.keras.Input((32, 32, 3)))\n",
    "\n",
    "parsed_model.summary()\n",
    "#%tensorboard --logdir logs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T18:23:59.459685Z",
     "end_time": "2023-08-29T18:24:00.490324Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With intermediate layers the growing and pruning of neurons changes only marginally. The pruning statement is exactly the same and in the growing statement we need to specify initializers for the intermediary layers. Note the doubly nested list for that, because there could be multiple intermediary layers. We add two neurons to the layer x1 and remove four neurons from layer x15. We need to retrieve the new grow prun tuples, sequential branches and parallel branches again after each adaption, because they refer to old layers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the layer of the first gp tuple: x1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " x1 (AdaptionLayer)             (None, 32, 32, 8)    224         ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x2 (AdaptionLayer)             (None, 32, 32, 8)    32          ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (AdaptionLayer)             (None, 15, 15, 6)    438         ['x2[0][0]']                     \n",
      "                                                                                                  \n",
      " x4 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      " x5 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x8 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x6 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x5[0][0]']                     \n",
      "                                                                                                  \n",
      " x9 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x8[0][0]']                     \n",
      "                                                                                                  \n",
      " x7 (AdaptionLayer)             (None, 7, 7, 6)      330         ['x6[0][0]']                     \n",
      "                                                                                                  \n",
      " x10 (AdaptionLayer)            (None, 7, 7, 6)      330         ['x9[0][0]']                     \n",
      "                                                                                                  \n",
      " x11 (Add)                      (None, 7, 7, 6)      0           ['x7[0][0]',                     \n",
      "                                                                  'x10[0][0]']                    \n",
      "                                                                                                  \n",
      " x12 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x11[0][0]']                    \n",
      "                                                                                                  \n",
      " x13 (AdaptionLayer)            (None, 7, 7, 9)      495         ['x12[0][0]']                    \n",
      "                                                                                                  \n",
      " x14 (AdaptionLayer)            (None, 7, 7, 9)      36          ['x13[0][0]']                    \n",
      "                                                                                                  \n",
      " x15 (AdaptionLayer)            (None, 3, 3, 6)      492         ['x14[0][0]']                    \n",
      "                                                                                                  \n",
      " x16 (Flatten)                  (None, 54)           0           ['x15[0][0]']                    \n",
      "                                                                                                  \n",
      " x17 (AdaptionLayer)            (None, 2)            110         ['x16[0][0]']                    \n",
      "                                                                                                  \n",
      " x18 (Softmax)                  (None, 2)            0           ['x17[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,243\n",
      "Trainable params: 3,161\n",
      "Non-trainable params: 82\n",
      "__________________________________________________________________________________________________\n",
      "Name of the layer of the third gp tuple: x13\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " x1 (AdaptionLayer)             (None, 32, 32, 8)    224         ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x2 (AdaptionLayer)             (None, 32, 32, 8)    32          ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (AdaptionLayer)             (None, 15, 15, 6)    438         ['x2[0][0]']                     \n",
      "                                                                                                  \n",
      " x4 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      " x5 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x8 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x6 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x5[0][0]']                     \n",
      "                                                                                                  \n",
      " x9 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x8[0][0]']                     \n",
      "                                                                                                  \n",
      " x7 (AdaptionLayer)             (None, 7, 7, 6)      330         ['x6[0][0]']                     \n",
      "                                                                                                  \n",
      " x10 (AdaptionLayer)            (None, 7, 7, 6)      330         ['x9[0][0]']                     \n",
      "                                                                                                  \n",
      " x11 (Add)                      (None, 7, 7, 6)      0           ['x7[0][0]',                     \n",
      "                                                                  'x10[0][0]']                    \n",
      "                                                                                                  \n",
      " x12 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x11[0][0]']                    \n",
      "                                                                                                  \n",
      " x13 (AdaptionLayer)            (None, 7, 7, 6)      330         ['x12[0][0]']                    \n",
      "                                                                                                  \n",
      " x14 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x13[0][0]']                    \n",
      "                                                                                                  \n",
      " x15 (AdaptionLayer)            (None, 3, 3, 6)      330         ['x14[0][0]']                    \n",
      "                                                                                                  \n",
      " x16 (Flatten)                  (None, 54)           0           ['x15[0][0]']                    \n",
      "                                                                                                  \n",
      " x17 (AdaptionLayer)            (None, 2)            110         ['x16[0][0]']                    \n",
      "                                                                                                  \n",
      " x18 (Softmax)                  (None, 2)            0           ['x17[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,904\n",
      "Trainable params: 2,828\n",
      "Non-trainable params: 76\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Name of the layer of the first gp tuple: \" + gp_tuples[0].first_layer.name)\n",
    "first_tuple = gp_tuples[0]\n",
    "\n",
    "init_units_kernel = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "init_units_bias = np.array([2, 2])\n",
    "\n",
    "inits_intermediate = [tf.keras.initializers.Ones(), tf.keras.initializers.Ones(), tf.keras.initializers.Ones(), tf.keras.initializers.Ones()]\n",
    "\n",
    "init_inputs = [tf.keras.initializers.Ones(), tf.keras.initializers.Ones()]\n",
    "init_dict = dict(first=[init_units_kernel, init_units_bias], intermediate=[inits_intermediate], last=init_inputs)\n",
    "parsed_model.grow(first_tuple, 2, init_dict, None, compile_fn)\n",
    "parsed_model.summary()\n",
    "\n",
    "gp_tuples = parsed_model.grow_prun_tuples\n",
    "sequential_branches = parsed_model.sequential_branches\n",
    "parallel_branches = parsed_model.parallel_branches\n",
    "\n",
    "print(\"Name of the layer of the third gp tuple: \" + gp_tuples[2].first_layer.name)\n",
    "third_tuple = gp_tuples[2]\n",
    "parsed_model.prun(third_tuple, [0, 2, 5], None, compile_fn)\n",
    "parsed_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T18:24:00.360953Z",
     "end_time": "2023-08-29T18:24:01.912300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " In the basic structure adaption example we saw sequential growing and pruning of layers. Now we will grow and prun parallel layers. First we remove on of the existing branches in the parallel branch. Note in the summary that the add node x11 has also been removed, since it only had one remaining input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the intermediate layers of the third sequential branch: ['x5', 'x6', 'x7']\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " x0 (InputLayer)             [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " x1 (AdaptionLayer)          (None, 32, 32, 8)         224       \n",
      "                                                                 \n",
      " x2 (AdaptionLayer)          (None, 32, 32, 8)         32        \n",
      "                                                                 \n",
      " x3 (AdaptionLayer)          (None, 15, 15, 6)         438       \n",
      "                                                                 \n",
      " x4 (AdaptionLayer)          (None, 15, 15, 6)         24        \n",
      "                                                                 \n",
      " x8 (AdaptionLayer)          (None, 15, 15, 6)         330       \n",
      "                                                                 \n",
      " x9 (AdaptionLayer)          (None, 15, 15, 6)         24        \n",
      "                                                                 \n",
      " x10 (AdaptionLayer)         (None, 7, 7, 6)           330       \n",
      "                                                                 \n",
      " x12 (AdaptionLayer)         (None, 7, 7, 6)           24        \n",
      "                                                                 \n",
      " x13 (AdaptionLayer)         (None, 7, 7, 6)           330       \n",
      "                                                                 \n",
      " x14 (AdaptionLayer)         (None, 7, 7, 6)           24        \n",
      "                                                                 \n",
      " x15 (AdaptionLayer)         (None, 3, 3, 6)           330       \n",
      "                                                                 \n",
      " x16 (Flatten)               (None, 54)                0         \n",
      "                                                                 \n",
      " x17 (AdaptionLayer)         (None, 2)                 110       \n",
      "                                                                 \n",
      " x18 (Softmax)               (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,220\n",
      "Trainable params: 2,156\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequential_branches = parsed_model.sequential_branches\n",
    "print(\"Name of the intermediate layers of the third sequential branch: \" + str([intermediate_layer.name for intermediate_layer in sequential_branches[1].intermediate_layers]))\n",
    "parsed_model.prun_branch(sequential_branches[1], None, compile_fn)\n",
    "parsed_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T18:24:01.691440Z",
     "end_time": "2023-08-29T18:24:02.486514Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we add a parallel branch again with different layers. This time we add right in front of the Flatten layer x16, starting from layer x12. Note that the framework introduces a new merge node under the name `new_add_node0`. The InsertBranch was designed to match the output shape of the other input Branch to the merge layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the layer 8: x12\n",
      "Name of the layer 12: x16\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " x0 (InputLayer)                [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " x1 (AdaptionLayer)             (None, 32, 32, 8)    224         ['x0[0][0]']                     \n",
      "                                                                                                  \n",
      " x2 (AdaptionLayer)             (None, 32, 32, 8)    32          ['x1[0][0]']                     \n",
      "                                                                                                  \n",
      " x3 (AdaptionLayer)             (None, 15, 15, 6)    438         ['x2[0][0]']                     \n",
      "                                                                                                  \n",
      " x4 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x3[0][0]']                     \n",
      "                                                                                                  \n",
      " x8 (AdaptionLayer)             (None, 15, 15, 6)    330         ['x4[0][0]']                     \n",
      "                                                                                                  \n",
      " x9 (AdaptionLayer)             (None, 15, 15, 6)    24          ['x8[0][0]']                     \n",
      "                                                                                                  \n",
      " x10 (AdaptionLayer)            (None, 7, 7, 6)      330         ['x9[0][0]']                     \n",
      "                                                                                                  \n",
      " x12 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x10[0][0]']                    \n",
      "                                                                                                  \n",
      " x13 (AdaptionLayer)            (None, 7, 7, 6)      330         ['x12[0][0]']                    \n",
      "                                                                                                  \n",
      " x19 (Dense)                    (None, 7, 7, 6)      42          ['x12[0][0]']                    \n",
      "                                                                                                  \n",
      " x14 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x13[0][0]']                    \n",
      "                                                                                                  \n",
      " x20 (AdaptionLayer)            (None, 7, 7, 6)      24          ['x19[0][0]']                    \n",
      "                                                                                                  \n",
      " x15 (AdaptionLayer)            (None, 3, 3, 6)      330         ['x14[0][0]']                    \n",
      "                                                                                                  \n",
      " x21 (AdaptionLayer)            (None, 3, 3, 6)      330         ['x20[0][0]']                    \n",
      "                                                                                                  \n",
      " new_add_node0 (Add)            (None, 3, 3, 6)      0           ['x15[0][0]',                    \n",
      "                                                                  'x21[0][0]']                    \n",
      "                                                                                                  \n",
      " x16 (Flatten)                  (None, 54)           0           ['new_add_node0[0][0]']          \n",
      "                                                                                                  \n",
      " x17 (AdaptionLayer)            (None, 2)            110         ['x16[0][0]']                    \n",
      "                                                                                                  \n",
      " x18 (Softmax)                  (None, 2)            0           ['x17[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,616\n",
      "Trainable params: 2,540\n",
      "Non-trainable params: 76\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layers = parsed_model.internal_model.layers\n",
    "print(\"Name of the layer 8: \" + layers[8].name)\n",
    "start_layer = layers[8]\n",
    "print(\"Name of the layer 12: \" + layers[12].name)\n",
    "end_layer = layers[12]\n",
    "\n",
    "l19 = tf.keras.layers.Dense(units=6, activation='relu', name='x19')\n",
    "x19 = l19(start_layer.output)\n",
    "l20 = tf.keras.layers.BatchNormalization(name='x20')\n",
    "x20 = l20(x19)\n",
    "l21 = tf.keras.layers.Conv2D(6, 3, strides=2, padding='valid', activation='relu', name='x21')\n",
    "x21 = l21(x20)\n",
    "\n",
    "insert_branch = structure_adaption.InsertBranch([l19, l20, l21], end_layer)\n",
    "\n",
    "parsed_model.grow_branch(insert_branch, None, compile_fn)\n",
    "parsed_model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-29T18:24:02.379818Z",
     "end_time": "2023-08-29T18:24:03.692990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have removed and added neurons in conjunction with intermediate layers. And we have removed and inserted branches non-sequentially. For more details on limitations and caveats consult the documentation and the other examples."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
